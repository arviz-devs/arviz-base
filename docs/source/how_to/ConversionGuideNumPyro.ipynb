{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "(numpyro_conversion)=\n",
    "# Converting NumPyro objects to DataTree\n",
    "\n",
    "{class}`~datatree.DataTree` is the data format ArviZ relies on.\n",
    "\n",
    "This page covers multiple ways to generate a `DataTree` from NumPyro MCMC and SVI objects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "```{seealso}\n",
    "\n",
    "- Conversion from Python, numpy or pandas objects\n",
    "- {ref}`xarray_for_arviz` for an overview of `InferenceData` and its role within ArviZ.\n",
    "- {ref}`schema` describes the structure of `InferenceData` objects and the assumptions made by ArviZ to ease your exploratory analysis of Bayesian models.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "We will start by importing the required packages and defining the model. The famous 8 school model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import arviz_base as az\n",
    "import numpy as np\n",
    "\n",
    "import numpyro\n",
    "import numpyro.distributions as dist\n",
    "from numpyro.infer import MCMC, NUTS, SVI, Trace_ELBO, autoguide\n",
    "from jax import random\n",
    "import jax.numpy as jnp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "J = 8\n",
    "y_obs = np.array([28.0, 8.0, -3.0, 7.0, -1.0, 1.0, 18.0, 12.0])\n",
    "sigma = np.array([15.0, 10.0, 16.0, 11.0, 9.0, 11.0, 10.0, 18.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eight_schools_model(J, sigma, y=None):\n",
    "    mu = numpyro.sample(\"mu\", dist.Normal(0, 5))\n",
    "    tau = numpyro.sample(\"tau\", dist.HalfCauchy(5))\n",
    "    with numpyro.plate(\"J\", J):\n",
    "        eta = numpyro.sample(\"eta\", dist.Normal(0, 1))\n",
    "        theta = numpyro.deterministic(\"theta\", mu + tau * eta)\n",
    "        return numpyro.sample(\"obs\", dist.Normal(theta, sigma), obs=y)\n",
    "    \n",
    "\n",
    "def eight_schools_custom_guide(J, sigma, y=None):\n",
    "\n",
    "    # Variational parameters for mu\n",
    "    mu_loc = numpyro.param(\"mu_loc\", 0.0)\n",
    "    mu_scale = numpyro.param(\"mu_scale\", 1.0, constraint=dist.constraints.positive)\n",
    "    mu = numpyro.sample(\"mu\", dist.Normal(mu_loc, mu_scale))\n",
    "\n",
    "    # Variational parameters for tau (positive support)\n",
    "    tau_loc = numpyro.param(\"tau_loc\", 1.0)\n",
    "    tau_scale = numpyro.param(\"tau_scale\", 0.5, constraint=dist.constraints.positive)\n",
    "    tau = numpyro.sample(\"tau\", dist.LogNormal(jnp.log(tau_loc), tau_scale))\n",
    "\n",
    "    # Variational parameters for eta\n",
    "    eta_loc = numpyro.param(\"eta_loc\", jnp.zeros(J))\n",
    "    eta_scale = numpyro.param(\"eta_scale\", jnp.ones(J), constraint=dist.constraints.positive)\n",
    "    with numpyro.plate(\"J\", J):\n",
    "        eta = numpyro.sample(\"eta\", dist.Normal(eta_loc, eta_scale))\n",
    "\n",
    "        # Deterministic transform\n",
    "        numpyro.deterministic(\"theta\", mu + tau * eta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## Convert from MCMC\n",
    "\n",
    "This first example shows conversion from MCMC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit with MCMC\n",
    "nuts = NUTS(eight_schools_model)\n",
    "mcmc = MCMC(nuts, num_warmup = 1000, num_samples = 1000, num_chains=4)\n",
    "mcmc.run(random.PRNGKey(0), J=J, sigma=sigma, y=y_obs, extra_fields=(\"num_steps\", \"energy\"),)\n",
    "\n",
    "# Convert to MCMC\n",
    "idata_mcmc = az.from_numpyro(mcmc)\n",
    "idata_mcmc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## Convert from SVI with Autoguide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "eight_schools_guide = autoguide.AutoNormal(eight_schools_model, init_loc_fn=numpyro.infer.init_to_median(num_samples=100))\n",
    "svi = SVI(\n",
    "    eight_schools_model, \n",
    "    guide=eight_schools_guide,\n",
    "    optim=numpyro.optim.Adam(0.01),\n",
    "    loss = Trace_ELBO()\n",
    ")\n",
    "svi_result = svi.run(random.PRNGKey(0), num_steps=10000, J=J, sigma=sigma, y=y_obs)\n",
    "idata_svi = az.from_numpyro_svi(\n",
    "    svi,\n",
    "    svi_result=svi_result,\n",
    "    model_kwargs=dict(J=J, sigma=sigma, y=y_obs), # SVI requires providing the fit args/kwargs\n",
    "    num_samples = 4000 # number of samples to draw in the posterior\n",
    ")\n",
    "idata_svi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## Converting from SVI with a custom guide function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "svi_custom_guide = SVI(\n",
    "    eight_schools_model, \n",
    "    guide=eight_schools_custom_guide,\n",
    "    optim=numpyro.optim.Adam(0.01),\n",
    "    loss = Trace_ELBO()\n",
    ")\n",
    "svi_custom_guide_result = svi_custom_guide.run(random.PRNGKey(0), num_steps=10000, J=J, sigma=sigma, y=y_obs)\n",
    "\n",
    "idata_svi_custom_guide = az.from_numpyro_svi(\n",
    "    svi_custom_guide,\n",
    "    svi_result=svi_custom_guide_result,\n",
    "    model_kwargs=dict(J=J, sigma=sigma, y=y_obs), # SVI requires providing the fit args/kwargs\n",
    "    num_samples = 4000 # number of samples to draw in the posterior\n",
    ")\n",
    "idata_svi_custom_guide"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "(emcee_conversion)=\n",
    "# Converting emcee objects to DataTree\n",
    "\n",
    "{class}`~datatree.DataTree` is the data format ArviZ relies on.\n",
    "\n",
    "This page covers multiple ways to generate a `DataTree` from emcee objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_prior_8school(theta):\n",
    "    mu, tau, eta = theta[0], theta[1], theta[2:]\n",
    "    # Half-cauchy prior, hwhm=25\n",
    "    if tau < 0:\n",
    "        return -np.inf\n",
    "    prior_tau = -np.log(tau**2 + 25**2)\n",
    "    prior_mu = -((mu / 10) ** 2)  # normal prior, loc=0, scale=10\n",
    "    prior_eta = -np.sum(eta**2)  # normal prior, loc=0, scale=1\n",
    "    return prior_mu + prior_tau + prior_eta\n",
    "\n",
    "\n",
    "def log_likelihood_8school(theta, y, s):\n",
    "    mu, tau, eta = theta[0], theta[1], theta[2:]\n",
    "    return -(((mu + tau * eta - y) / s) ** 2)\n",
    "\n",
    "\n",
    "def lnprob_8school(theta, y, s):\n",
    "    prior = log_prior_8school(theta)\n",
    "    like_vect = log_likelihood_8school(theta, y, s)\n",
    "    like = np.sum(like_vect)\n",
    "    return like + prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "nwalkers = 40  # called chains in ArviZ\n",
    "ndim = J + 2\n",
    "draws = 1500\n",
    "pos = np.random.normal(size=(nwalkers, ndim))\n",
    "pos[:, 1] = np.absolute(pos[:, 1])\n",
    "sampler = emcee.EnsembleSampler(nwalkers, ndim, lnprob_8school, args=(y_obs, sigma))\n",
    "sampler.run_mcmc(pos, draws);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "## Manually set variable names\n",
    "This first example will show how to convert manually setting the variable names only, leaving everything else to ArviZ defaults."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define variable names, it cannot be inferred from emcee\n",
    "var_names = [\"mu\", \"tau\"] + [\"eta{}\".format(i) for i in range(J)]\n",
    "idata1 = az.from_emcee(sampler, var_names=var_names)\n",
    "idata1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "ArviZ has stored the posterior variables with the provided names as expected, but it has also included other useful information in the `InferenceData` object. The log probability of each sample is stored in the `sample_stats` group under the name `lp` and all the arguments passed to the sampler as `args` have been saved in the `observed_data` group."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "It can also be useful to perform a burn in cut to the MCMC samples (see {meth}`arviz.InferenceData.sel` for more details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "#idata1.sel(draw=slice(100, None))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "From an InferenceData object, ArviZ's native data structure, the {func}`posterior plot <arviz.plot_posterior>` of a few variables can be done in one line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#az.plot_posterior(idata1, var_names=[\"mu\", \"tau\", \"eta4\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "## Structuring the posterior as multidimensional variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "This way of calling ``from_emcee`` stores each `eta` as a different variable, called `eta#`, \n",
    "however, they are in fact different dimensions of the same variable. \n",
    "This can be seen in the code of the likelihood and prior functions, where `theta` is unpacked as:\n",
    "\n",
    "    mu, tau, eta = theta[0], theta[1], theta[2:]\n",
    "    \n",
    "ArviZ has support for multidimensional variables, and there is a way to tell it how to split the variables like it was done in the likelihood and prior functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "idata2 = az.from_emcee(sampler, slices=[0, 1, slice(2, None)])\n",
    "idata2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "After checking the default variable names, the trace of one dimension of eta can be plotted using ArviZ syntax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#az.plot_trace(idata2, var_names=[\"var_2\"], coords={\"var_2_dim_0\": 4});"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "## `blobs`: unlock sample stats, posterior predictive and miscellanea"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "Emcee does not store per-draw sample stats, however, it has a functionality called\n",
    "blobs that allows to store any variable on a per-draw basis. It can be used\n",
    "to store some sample_stats or even posterior_predictive data. \n",
    "\n",
    "You can modify the probability function to use this ``blobs`` functionality and store the pointwise log likelihood,\n",
    "then rerun the sampler using the new function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lnprob_8school_blobs(theta, y, s):\n",
    "    prior = log_prior_8school(theta)\n",
    "    like_vect = log_likelihood_8school(theta, y, s)\n",
    "    like = np.sum(like_vect)\n",
    "    return like + prior, like_vect\n",
    "\n",
    "\n",
    "sampler_blobs = emcee.EnsembleSampler(\n",
    "    nwalkers,\n",
    "    ndim,\n",
    "    lnprob_8school_blobs,\n",
    "    args=(y_obs, sigma),\n",
    ")\n",
    "sampler_blobs.run_mcmc(pos, draws);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "You can now use the `blob_names` argument to indicate how to store this blob-defined variable. As the group is not specified, it will go to sample_stats.\n",
    "Note that the argument blob_names is added to the arguments covered in the previous examples and we are also introducing `coords` and `dims` arguments to show the power and flexibility of the converter. For more on `coords` and `dims` see `page_in_construction`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "dims = {\"eta\": [\"school\"], \"log_likelihood\": [\"school\"]}\n",
    "idata3 = az.from_emcee(\n",
    "    sampler_blobs,\n",
    "    var_names=[\"mu\", \"tau\", \"eta\"],\n",
    "    slices=[0, 1, slice(2, None)],\n",
    "    blob_names=[\"y\"],\n",
    "    dims=dims,\n",
    "    coords={\"school\": range(8)},\n",
    ")\n",
    "idata3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {},
   "source": [
    "## Multi-group blobs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {},
   "source": [
    "You might even have more complicated blobs, each corresponding to a different group of the InferenceData object. Moreover, you can store the variables passed to the ``EnsembleSampler`` via the ``args`` argument in observed or constant data groups. This is shown in the example below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler_blobs.blobs[0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lnprob_8school_blobs(theta, y, sigma):\n",
    "    mu, tau, eta = theta[0], theta[1], theta[2:]\n",
    "    prior = log_prior_8school(theta)\n",
    "    like_vect = log_likelihood_8school(theta, y, sigma)\n",
    "    like = np.sum(like_vect)\n",
    "    # store pointwise log likelihood, useful for model comparison with az.loo or az.waic\n",
    "    # and posterior predictive samples as blobs\n",
    "    return like + prior, (like_vect, np.random.normal((mu + tau * eta), sigma))\n",
    "\n",
    "\n",
    "sampler_blobs = emcee.EnsembleSampler(\n",
    "    nwalkers,\n",
    "    ndim,\n",
    "    lnprob_8school_blobs,\n",
    "    args=(y_obs, sigma),\n",
    ")\n",
    "sampler_blobs.run_mcmc(pos, draws)\n",
    "\n",
    "dims = {\"eta\": [\"school\"], \"log_likelihood\": [\"school\"], \"y\": [\"school\"]}\n",
    "idata4 = az.from_emcee(\n",
    "    sampler_blobs,\n",
    "    var_names=[\"mu\", \"tau\", \"eta\"],\n",
    "    slices=[0, 1, slice(2, None)],\n",
    "    arg_names=[\"y\", \"sigma\"],\n",
    "    arg_groups=[\"observed_data\", \"constant_data\"],\n",
    "    blob_names=[\"y\", \"y\"],\n",
    "    blob_groups=[\"log_likelihood\", \"posterior_predictive\"],\n",
    "    dims=dims,\n",
    "    coords={\"school\": range(8)},\n",
    ")\n",
    "idata4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44",
   "metadata": {},
   "source": [
    "This last version, which contains both observed data and posterior predictive can be used to plot posterior predictive checks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#az.plot_ppc(idata4, var_names=[\"y\"], alpha=0.3, num_pp_samples=200);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext watermark\n",
    "%watermark -n -u -v -iv -w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arviz-dev312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
